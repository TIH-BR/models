# Copyright 2021 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# SimCLR Imagenet 10% finetuning.
runtime:
  distribution_strategy: 'mirrored'
  mixed_precision_dtype: 'float16'
  loss_scale: 'dynamic'
  num_gpus: 16
task:
  model:
    mode: 'finetune'
    input_size: [224, 224, 3]
    backbone:
      type: 'resnet'
      resnet:
        model_id: 50
    backbone_trainable: true
    projection_head:
      proj_output_dim: 128
      num_proj_layers: 3
      ft_proj_idx: 1
    supervised_head:
      num_classes: 1001
      zero_init: true
    norm_activation:
      use_sync_bn: false
      norm_momentum: 0.9
      norm_epsilon: 0.00001
  loss:
    label_smoothing: 0.0
    one_hot: true
  evaluation:
    top_k: 5
    one_hot: true
  init_checkpoint: gs://tf_model_garden/vision/simclr/r50_1x
  init_checkpoint_modules: 'backbone_projection'
  train_data:
    tfds_name: 'imagenet2012_subset/10pct'
    tfds_split: 'train'
    input_path: ''
    is_training: true
    global_batch_size: 1024
    dtype: 'float16'
    parser:
      mode: 'finetune'
  validation_data:
    tfds_name: 'imagenet2012_subset/10pct'
    tfds_split: 'validation'
    input_path: ''
    is_training: false
    global_batch_size: 1024
    dtype: 'float16'
    drop_remainder: false
    parser:
      mode: 'finetune'
trainer:
  train_steps: 12500  # 100 epochs
  validation_steps: 49  # NUM_EXAMPLES (50000) // global_batch_size
  validation_interval: 125
  steps_per_loop: 125  # NUM_EXAMPLES (128116) // global_batch_size
  summary_interval: 125
  checkpoint_interval: 125
  optimizer_config:
    optimizer:
      type: 'lars'
      lars:
        momentum: 0.9
        weight_decay_rate: 0.0
        exclude_from_weight_decay: ['batch_normalization', 'bias']
    learning_rate:
      type: 'cosine'
      cosine:
        initial_learning_rate: 0.04  #  0.01 Ã— BatchSize / 512
        decay_steps: 12500  # train_steps
